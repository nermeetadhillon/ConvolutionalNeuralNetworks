{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import utils\n",
    "import importlib\n",
    "importlib.reload(utils)\n",
    "\n",
    "# PyTorch is needed for this assignment\n",
    "# You can install it following the instructions on the official website: https://pytorch.org/get-started/locally/\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(0)\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "Load the sign language dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "X_train, Y_train, X_test, Y_test = utils.load_data()\n",
    "\n",
    "print('X_train.shape:', X_train.shape)\n",
    "print('Y_train.shape:', Y_train.shape)\n",
    "print('X_test.shape:', X_test.shape)\n",
    "print('Y_test.shape:', Y_test.shape)\n",
    "\n",
    "# Visualize some data\n",
    "fig = plt.figure(figsize=(16, 8))\n",
    "for i in range(10):\n",
    "    img = X_train[180*i,:].reshape((64,64))\n",
    "    fig.add_subplot(1, 10, i+1)\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    y_label = Y_train[180*i]\n",
    "    plt.title(y_label)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Task 1 Define the model\n",
    "\n",
    "\n",
    "You will need to define your CNN model as a subclass of `torch.nn.Module`. Becuase we have already imported `torch.nn` as `nn`, we can specify the baseclass simply as `nn.Module`. \n",
    "\n",
    "You need to override two functions in defining the class, `__init__()` and `forward()`.\n",
    "- All the parameters, including the convolutional, pooling, and fully-connected layers are defined in `__init__()`. They are declared and initialized as members of the class, using the `self.` notation in Python. \n",
    "- The forward pass of the computational graph is defined in `forward()`. This function takes as input the training data, and call all operations (conv, pool, etc.) sequentially on the data. The output of a preceding operation is used as the input for the following operation. \n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- Define the model so that the architecture is as follows: <br>\n",
    "CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED_1 -> FULLYCONNECTED_2 -> SOFTMAX.\n",
    "\n",
    "- *Note* that the *RELU* and *FLATTEN* functions are defined in `forward()` rather than `__init__()`.\n",
    "\n",
    "- The `in_features` of `self.fc1` is the total number of output units after the `self.pool2` layer. This can be computed using the formula in the slides:\n",
    "\\begin{equation}\\text{Output} = (\\lfloor\\frac{n+2p-f}{s}\\rfloor + 1)\\times(\\lfloor\\frac{n+2p-f}{s}\\rfloor + 1)\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_Model, self).__init__()\n",
    "        \n",
    "        # The first convolutional layer has in_channels=1, out_channels=6, kernel_size=3, with default stride=1 and padding=0\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        # The first pooling layer is a maxpool with a square window of kernel_size=2 (default stride is same as kernel_size)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        \n",
    "        ### START YOUR CODE ###\n",
    "        # The second convolutional layer's in_channels should match the out_channels of conv1\n",
    "        # Hint: Replace the \"None\" with the correct number\n",
    "        self.conv2 = nn.Conv2d(in_channels = 6, out_channels = 12, kernel_size = 5, stride=2)\n",
    "        \n",
    "        # The second pooling layer has the same setting as pool1\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        \n",
    "        # The first fully-connected layer\n",
    "        # Hint: Use nn.Linear, and replace the \"None\" with the correct number for the in_features\n",
    "        \n",
    "        self.fc1 = nn.Linear(588, 100)\n",
    "        ### END YOUR CODE ###\n",
    "        \n",
    "        # Second fully-connected layer\n",
    "        self.fc2 = nn.Linear(100, 10)\n",
    "        \n",
    "        # Softmax layer\n",
    "        self.output = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Conv1 -> ReLU -> Pool1\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        \n",
    "        ### START YOUR CODE ###\n",
    "        # Conv2 -> ReLU -> Pool2\n",
    "        # Hint: Follow the line of code above\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        \n",
    "        # Flatten the output from the last pooling layer\n",
    "        # Hint: Replace \"None\" with the correct number (same as the in_features of fc1)\n",
    "        x = x.view(-1, 588)\n",
    "        ### END YOUR CODE ###\n",
    "        \n",
    "        # Call two fully-connected layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        # Call softmax layer\n",
    "        x = self.output(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Task 1\n",
    "model = CNN_Model()\n",
    "\n",
    "torch.manual_seed(0)\n",
    "input_data = torch.randn(20, 1, 64, 64)\n",
    "output = model(input_data)\n",
    "\n",
    "print('output.size():', output.size())\n",
    "print('output.sum()', output.sum().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output\n",
    "\n",
    "|&nbsp;|&nbsp; |          \n",
    "|--|--|\n",
    "|**output.size():**| torch.Size([20, 10])|\n",
    "|**output.sum():**| -460.79412841796875|\n",
    "\n",
    "***\n",
    "\n",
    "## Task 2 Train and evaluation\n",
    "\n",
    "\n",
    "Now you will use the functions you have implemented above to build a full model. Then you train the model on the sign language dataset.\n",
    "\n",
    "You can refer to the official website of PyTorch (https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html) for information about how to use optimizers, computing loss, and carrying out backprop etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(model, X_train, Y_train, X_test, Y_test, learning_rate=0.01,\n",
    "          num_epochs=100, minibatch_size=8, print_cost=True):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    X_train -- training set, of shape (m, 1, 64, 64), in which m is the number of training examples\n",
    "    Y_train -- test set, of shape (m, 10)\n",
    "    X_test -- training set, of shape (m', 1, 64, 64)\n",
    "    Y_test -- test set, of shape (m', 10)\n",
    "    learning_rate -- learning rate \n",
    "    num_epochs -- number of epochs\n",
    "    minibatch_size -- size of a minibatch\n",
    "    print_cost -- True to print the cost every 5 epochs\n",
    "    \"\"\"\n",
    "    \n",
    "    seed = 0                                         # to keep results consistent (numpy seed)\n",
    "    (m, n_H0, n_W0, n_C0) = X_train.shape                                  \n",
    "    costs = []                                       # To keep track of the cost\n",
    "    \n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "     \n",
    "    # Training loop\n",
    "    model.train() # Turn on the training mode\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        minibatch_cost = 0.\n",
    "        num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "        seed = seed + 1\n",
    "        minibatches = utils.generate_minibatch(X_train, Y_train, minibatch_size, seed)\n",
    "\n",
    "        for minibatch in minibatches:\n",
    "            (batch_x, batch_y) = minibatch\n",
    "            data_x = torch.Tensor(batch_x)\n",
    "            data_y = torch.LongTensor(batch_y)\n",
    "            \n",
    "            ### START YOUR CODE ### \n",
    "           # Zero the gradients\n",
    "            optimizer.zero_grad() # Hint: call zero_grad()\n",
    "            \n",
    "            # Forward pass and compute loss\n",
    "            outputs = model(data_x) # Hint: use model as a callable\n",
    "            loss = criterion(outputs, data_y) #criterion() # Hint: use criterion as a callable\n",
    "            \n",
    "            # Backward and optimize\n",
    "            loss.backward() # Hint: call backward()\n",
    "            optimizer.step() # Hint: call step()\n",
    "            ### END YOUR CODE ###\n",
    "            \n",
    "            minibatch_cost += loss.item()\n",
    "        \n",
    "        # Print the cost every epoch\n",
    "        minibatch_cost /= num_minibatches\n",
    "        if print_cost and epoch % 5 == 0:\n",
    "            print (\"Cost after epoch %i: %f\" % (epoch, minibatch_cost))\n",
    "        costs.append(minibatch_cost)\n",
    "        \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title('Learning rate =' + str(learning_rate))\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate accuracy on the train and test datasets\n",
    "    data_x = torch.Tensor(X_test)\n",
    "    data_y = torch.LongTensor(Y_test)\n",
    "    model.eval() # Turn on the evaluation mode\n",
    "    with torch.no_grad():\n",
    "        test_pred = model(data_x)\n",
    "        num_correct = (torch.argmax(test_pred, dim=1).view(data_y.size()).data == data_y.data).float().sum()\n",
    "        test_acc = num_correct / test_pred.size()[0]\n",
    "    print(\"Test Accuracy:\", test_acc.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Task 2\n",
    "model = CNN_Model()\n",
    "\n",
    "torch.manual_seed(0)\n",
    "run(model, X_train, Y_train, X_test, Y_test)\n",
    "\n",
    "# NOTE: It could be slow to run 100 epochs. Make sure that your costs for after each epoch  \n",
    "# are the same as those in the expected output. If yours are different, click the stop button\n",
    "# on the menu bar and then check your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output\n",
    "\n",
    "|&nbsp;|&nbsp; |          \n",
    "|--|--|\n",
    "|**Cost after epoch 0:**| 2.313738|\n",
    "|**Cost after epoch 5:**| 2.310775|\n",
    "|**Cost after epoch 10:**| 2.290986|\n",
    "|...|...|\n",
    "|**Cost after epoch 85:**| 0.002756|\n",
    "|**Cost after epoch 90:**| 0.002577|\n",
    "|**Cost after epoch 95:**| 0.002472|\n",
    "\n",
    "\n",
    "The performance varies each time you run the model.\n",
    "Roughly speaking, you should get a **test accuracy** around **90%** (Even above when you are lucky).\n",
    "\n",
    "***\n",
    "\n",
  
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
